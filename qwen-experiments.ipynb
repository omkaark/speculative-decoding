{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054db4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omkaarwork/Desktop/projects/speculative-decoding/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "from safetensors.torch import load_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE\n",
    "def rotate_half(t: torch.Tensor) -> torch.Tensor:\n",
    "    d2 = t.shape[-1] // 2\n",
    "    return torch.cat((-t[..., d2:], t[..., :d2]), dim=-1)\n",
    "\n",
    "def apply_rope(qk: torch.Tensor, base: float, token_idx: int) -> torch.Tensor:\n",
    "    _, _, S, D = qk.shape\n",
    "    assert D % 2 == 0\n",
    "    inv = torch.arange(0, D, 2, device=qk.device, dtype=torch.float32) / D\n",
    "    inv_freq = base ** (-inv) # (D/2,)\n",
    "    t = torch.arange(token_idx, token_idx+S, device=qk.device, dtype=torch.float32) # (S,)\n",
    "    freqs = torch.einsum(\"s,d->sd\", t, inv_freq) # (S, D/2)\n",
    "    emb = torch.cat([freqs, freqs], dim=-1) # (S, D)\n",
    "    cos = emb.cos().to(qk.dtype)[None, None, :, :] # (1,1,S,D)\n",
    "    sin = emb.sin().to(qk.dtype)[None, None, :, :]\n",
    "    return (qk * cos) + (rotate_half(qk) * sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = torch.device('mps')\n",
    "        self.model = load_file(f\"{model_path}/model.safetensors\", device=self.device.type)\n",
    "\n",
    "        # Tokenize and embed\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"{model_path}\")\n",
    "        with open(f\"{model_path}/config.json\", \"r\") as f:\n",
    "            self.config = json.load(f)\n",
    "            \n",
    "        self.embed = torch.nn.Embedding(self.config[\"vocab_size\"], self.config[\"hidden_size\"], device='mps', dtype=torch.bfloat16)\n",
    "        self.embed.load_state_dict({\"weight\": self.model[\"model.embed_tokens.weight\"]})\n",
    "\n",
    "        # Set KV Cache\n",
    "        SEQ_LEN = 1024\n",
    "        self.kv_cache = [{\n",
    "            \"key\": torch.empty((1, self.config[\"num_attention_heads\"], SEQ_LEN, self.config[\"hidden_size\"] // self.config[\"num_attention_heads\"]), dtype=torch.bfloat16, device=self.device),\n",
    "            \"value\": torch.empty((1, self.config[\"num_attention_heads\"], SEQ_LEN, self.config[\"hidden_size\"] // self.config[\"num_attention_heads\"]), dtype=torch.bfloat16, device=self.device),\n",
    "        } for _ in range(self.config[\"num_hidden_layers\"])]\n",
    "\n",
    "    def forward_logits(self, x, token_idx):\n",
    "        S = x.shape[1]\n",
    "        hidden = self.config[\"hidden_size\"]\n",
    "        n_heads = self.config[\"num_attention_heads\"]\n",
    "        n_kv = self.config[\"num_key_value_heads\"]\n",
    "        head_dim = hidden // n_heads\n",
    "        for layer in range(self.config[\"num_hidden_layers\"]):\n",
    "            # RMSNorm\n",
    "            x_rms = torch.nn.functional.rms_norm(\n",
    "                x, normalized_shape=(hidden,),\n",
    "                weight=self.model[f\"model.layers.{layer}.input_layernorm.weight\"],\n",
    "                eps=self.config[\"rms_norm_eps\"],\n",
    "            ).to(torch.bfloat16)\n",
    "\n",
    "            # QKV\n",
    "            q = x_rms @ self.model[f\"model.layers.{layer}.self_attn.q_proj.weight\"].T + self.model[f\"model.layers.{layer}.self_attn.q_proj.bias\"]\n",
    "            k = x_rms @ self.model[f\"model.layers.{layer}.self_attn.k_proj.weight\"].T + self.model[f\"model.layers.{layer}.self_attn.k_proj.bias\"]\n",
    "            v = x_rms @ self.model[f\"model.layers.{layer}.self_attn.v_proj.weight\"].T + self.model[f\"model.layers.{layer}.self_attn.v_proj.bias\"]\n",
    "\n",
    "            q = q.view(1, S, n_heads, head_dim).transpose(1, 2) # [1,H,S,D]\n",
    "            k = k.view(1, S, n_kv, head_dim).transpose(1, 2) # [1,KV,S,D]\n",
    "            v = v.view(1, S, n_kv, head_dim).transpose(1, 2)\n",
    "\n",
    "            if n_heads != n_kv:\n",
    "                reps = n_heads // n_kv\n",
    "                k = k.repeat_interleave(reps, dim=1)\n",
    "                v = v.repeat_interleave(reps, dim=1)\n",
    "\n",
    "            # RoPE\n",
    "            theta = self.config[\"rope_theta\"]\n",
    "            q = apply_rope(q, theta, token_idx=token_idx)\n",
    "            k = apply_rope(k, theta, token_idx=token_idx)\n",
    "\n",
    "            # KV cache\n",
    "            self.kv_cache[layer][\"key\"][:, :, token_idx:token_idx+S] = k # B, H, s:s+x, D\n",
    "            self.kv_cache[layer][\"value\"][:, :, token_idx:token_idx+S] = v # B, H, s:s+x, D\n",
    "\n",
    "            k = self.kv_cache[layer][\"key\"][:, :, :token_idx+S]\n",
    "            v = self.kv_cache[layer][\"value\"][:, :, :token_idx+S]\n",
    "\n",
    "            # attn\n",
    "            scores = (q @ k.transpose(-2, -1)) / (head_dim ** 0.5) # [1,H,S,S]\n",
    "            causal = torch.triu(torch.full((S, S), float(\"-inf\"), device='mps', dtype=scores.dtype), 1)\n",
    "            attn = torch.softmax(scores + causal, dim=-1)\n",
    "            attn_o = (attn @ v).transpose(1, 2).reshape(1, S, hidden) @ self.model[f\"model.layers.{layer}.self_attn.o_proj.weight\"].T\n",
    "\n",
    "            x = x + attn_o\n",
    "\n",
    "            # FFN\n",
    "            x_rms = torch.nn.functional.rms_norm(\n",
    "                x, normalized_shape=(hidden,),\n",
    "                weight=self.model[f\"model.layers.{layer}.post_attention_layernorm.weight\"],\n",
    "                eps=self.config[\"rms_norm_eps\"],\n",
    "            ).to(torch.bfloat16)\n",
    "\n",
    "            gate = x_rms @ self.model[f\"model.layers.{layer}.mlp.gate_proj.weight\"].T\n",
    "            up = x_rms @ self.model[f\"model.layers.{layer}.mlp.up_proj.weight\"].T\n",
    "            ffn = (torch.nn.functional.silu(gate) * up) @ self.model[f\"model.layers.{layer}.mlp.down_proj.weight\"].T\n",
    "\n",
    "            x = x + ffn\n",
    "\n",
    "        # final norm + logits\n",
    "        x_norm = torch.nn.functional.rms_norm(\n",
    "            x, normalized_shape=(self.config[\"hidden_size\"],),\n",
    "            weight=self.model[\"model.norm.weight\"],\n",
    "            eps=self.config[\"rms_norm_eps\"],\n",
    "        ).to(torch.bfloat16)\n",
    "\n",
    "        logits = x_norm @ self.model[\"model.embed_tokens.weight\"].T\n",
    "        return logits\n",
    "\n",
    "    def generate(self, messages):\n",
    "        rendered = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        tokens = self.tokenizer(rendered,  return_tensors=\"pt\", add_special_tokens=False).input_ids.to('mps')\n",
    "        x = self.embed(tokens)\n",
    "        generated_id = None\n",
    "        generated_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while generated_id == None or generated_id != self.tokenizer.eos_token_id:\n",
    "                # embed input tokens\n",
    "                x = self.embed(tokens)\n",
    "\n",
    "                # forward and get final logits\n",
    "                logits = self.forward_logits(x, token_idx=generated_tokens)\n",
    "\n",
    "                # decode\n",
    "                generated_id = int(logits[:, -1, :].argmax(dim=-1))\n",
    "                if generated_id == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                next_token = torch.tensor([[generated_id]], device=tokens.device, dtype=tokens.dtype)\n",
    "                \n",
    "                # append to tokens and output\n",
    "                generated_tokens += tokens.shape[1]\n",
    "                tokens = next_token.clone()\n",
    "                print(self.tokenizer.decode(generated_id, skip_special_tokens=True), end='', flush=True)\n",
    "\n",
    "Qwen2('./qwen2-0.5b').generate([\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"Ouptut 1 a hundred times\"\n",
    "    }\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speculative-decoding-llama3 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
